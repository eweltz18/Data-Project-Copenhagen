{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = pd.read_csv('home_completed.csv')\n",
    "away = pd.read_csv('away_completed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all data into one large data frame\n",
    "main = home.append(away, ignore_index=True).sort_values(['Checker']) # puts into date order\n",
    "#put columns into more organized order\n",
    "main = main[['Year','Month', 'Day' , 'Team', 'Spread', 'Line', '+/-', '3P', '3P%', '3PA', '3PAr', 'AST', 'BLK', 'DRB', 'Day',\n",
    "       'FG', 'FG%', 'FGA', 'FT', 'FT%', 'FTA', 'FTr', 'Home', 'Losses',\n",
    "        'ORB', 'PF', 'PTS', 'STL', 'Starters MP', 'TOV', 'TS%', 'Wins',\n",
    "        'eFG%',  'Checker']]\n",
    "#number of points a team won or lost by\n",
    "main['+/-'] = main['+/-']/5\n",
    "#puts the odds into floats\n",
    "main['Spread'] = main['Spread'].replace(' PK', '0').astype(float)\n",
    "#gets how many points a team went over or under the spread\n",
    "main['Cover'] = main['Spread'] + main['+/-']\n",
    "#true or false on whether a team covered the spread\n",
    "main['ATS'] = np.where(main['Cover'].isnull(), np.nan,\n",
    "          np.where(main['Cover'] > 0,   1, 0))\n",
    "teams = list(main['Team'])\n",
    "new_teams = []\n",
    "#adds the opponent to the df\n",
    "for i in range(0, len(teams), 2):\n",
    "    new_teams.append(teams[i+1])\n",
    "    new_teams.append(teams[i])\n",
    "main['Opponent'] = new_teams\n",
    "main = main.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_df(team):\n",
    "    t = main[main['Team']== team].reset_index()\n",
    "    del t['index']\n",
    "    t['Season'] = t['Checker'].apply(season) #define what season a game is played\n",
    "    t['Games Played'] = t['Wins'] + t['Losses'] #total games played\n",
    "    #create running averages for all stats up to a given game date\n",
    "    t_df = pd.DataFrame()\n",
    "    for year in years:\n",
    "        temp = t[t['Season']==year].reset_index()\n",
    "        temp['Avg Win/Loss'] = (temp['+/-'].cumsum() - temp['+/-'])/temp.index\n",
    "        temp['Avg 3P'] = (temp['3P'].cumsum() - temp['3P'])/temp.index\n",
    "        temp['Avg 3PA'] = (temp['3PA'].cumsum() - temp['3PA'])/temp.index\n",
    "        temp['Avg 3P%'] = (temp['Avg 3P']/temp['Avg 3PA'])\n",
    "        temp['Avg Starter MP'] = (temp['Starters MP'].cumsum() - temp['Starters MP'])/temp.index\n",
    "        temp['Avg AST'] = (temp['AST'].cumsum() - temp['AST'])/temp.index\n",
    "        temp['Avg BLK'] = (temp['BLK'].cumsum() - temp['BLK'])/temp.index\n",
    "        temp['Avg DRB'] = (temp['DRB'].cumsum() - temp['DRB'])/temp.index\n",
    "        temp['Avg ORB'] = (temp['ORB'].cumsum() - temp['ORB'])/temp.index\n",
    "        temp['Avg FG'] = (temp['FG'].cumsum() - temp['FG'])/temp.index\n",
    "        temp['Avg FGA'] = (temp['FGA'].cumsum() - temp['FGA'])/temp.index\n",
    "        temp['Avg FG%'] = (temp['Avg FG']/temp['Avg FGA'])\n",
    "        temp['Avg FT'] = (temp['FT'].cumsum() - temp['FT'])/temp.index\n",
    "        temp['Avg FTA'] = (temp['FTA'].cumsum() - temp['FTA'])/temp.index\n",
    "        temp['Avg 3PAr'] = (temp['Avg 3PA']/temp['Avg FGA'])\n",
    "        temp['Avg FTr'] = (temp['Avg FTA']/temp['Avg FGA'])\n",
    "        temp['Avg 3P%'] = (temp['Avg FT']/temp['Avg FTA'])/temp.index\n",
    "        temp['Avg PF'] = (temp['PF'].cumsum() - temp['PF'])/temp.index\n",
    "        temp['Avg PTS'] = (temp['PTS'].cumsum() - temp['PTS'])/temp.index\n",
    "        temp['Avg STL'] = (temp['STL'].cumsum() - temp['STL'])/temp.index\n",
    "        temp['Avg TOV'] = (temp['TOV'].cumsum() - temp['TOV'])/temp.index\n",
    "        temp['Avg TS%'] = temp['Avg PTS']/ (2*(temp['Avg FGA'] + .44 * temp['Avg FTA']))\n",
    "        temp['Avg eFG%'] = (temp['Avg FG'] + .5 * temp['Avg 3P'])/ temp['Avg FGA']\n",
    "        temp['Avg Starter MP'] = (temp['Starters MP'].cumsum() - temp['Starters MP'])/temp.index\n",
    "        temp['Winning Percentage'] = temp['Wins']/temp.index\n",
    "        t_df = t_df.append(temp)\n",
    "    return t_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put each row into a season\n",
    "def season(date):\n",
    "    date = date[:6]\n",
    "    if date < '200809':\n",
    "        return '2007'\n",
    "    elif date < '200909':\n",
    "        return '2008'\n",
    "    elif date < '201009':\n",
    "        return '2009'\n",
    "    elif date < '201109':\n",
    "        return '2010'\n",
    "    elif date < '201109':\n",
    "        return '2010'\n",
    "    elif date < '201209':\n",
    "        return '2011'\n",
    "    elif date < '201309':\n",
    "        return '2012'\n",
    "    elif date < '201409':\n",
    "        return '2013'\n",
    "    elif date < '201509':\n",
    "        return '2014'\n",
    "    elif date < '201609':\n",
    "        return '2015'\n",
    "    elif date < '201709':\n",
    "        return '2016'\n",
    "    elif date < '201809':\n",
    "        return '2017'\n",
    "    elif date < '201909':\n",
    "        return '2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = set(teams)\n",
    "years = ['2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018']\n",
    "all_team_data = {}\n",
    "for team in teams:\n",
    "    all_team_data[team] = team_df(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant_df = pd.DataFrame()\n",
    "for df in all_team_data.values():\n",
    "    giant_df = giant_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team in teams:\n",
    "    t = all_team_data[team]\n",
    "    opp = giant_df[giant_df['Opponent']==team]\n",
    "    opp = opp.rename(columns = {'Avg Win/Loss':'oppAvg Win/Loss', 'Avg 3P':'oppAvg 3P', 'Avg 3PA':'oppAvg 3PA' ,\n",
    "       'Avg 3P%':'oppAvg 3P%', 'Avg Starter MP':'oppAvg Starter MP', 'Avg AST':'oppAvg AST', 'Avg BLK':'oppAvg BLK', 'Avg DRB':'oppAvg DRB', 'Avg ORB':'oppAvg ORB',\n",
    "       'Avg FG':'oppAvg FG', 'Avg FGA':'oppAvg FGA', 'Avg FG%':'oppAvg FG%', 'Avg FT':'oppAvg FT', 'Avg FTA':'oppAvg FTA', 'Avg 3PAr':'oppAvg 3PAr',\n",
    "       'Avg FTr':'oppAvg FTr', 'Avg PF':'oppAvg PF', 'Avg PTS':'oppAvg PTS', 'Avg STL':'oppAvg STL', 'Avg TOV':'oppAvg TOV', 'Avg TS%':'oppAvg TS%',\n",
    "       'Avg eFG%':'oppAvg eFG%', 'Winning Percentage':'oppWinning Percentage'})\n",
    "    opp = opp[['Checker', 'oppAvg Win/Loss', 'oppAvg 3P', 'oppAvg 3PA',\n",
    "       'oppAvg 3P%', 'oppAvg Starter MP', 'oppAvg AST', 'oppAvg BLK',\n",
    "       'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG', 'oppAvg FGA', 'oppAvg FG%',\n",
    "       'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr', 'oppAvg FTr', 'oppAvg PF',\n",
    "       'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV', 'oppAvg TS%', 'oppAvg eFG%',\n",
    "       'oppWinning Percentage']]\n",
    "    all_team_data[team] = pd.merge(t,opp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = all_team_data['Philadelphia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_matrix = phi[['Cover', '+/-', 'Wins', 'Losses', 'Home', 'Avg Win/Loss', 'Avg 3P',\n",
    "       'Avg 3PA', 'Avg 3P%', 'Avg Starter MP', 'Avg AST', 'Avg BLK', 'Avg DRB',\n",
    "       'Avg ORB', 'Avg FG', 'Avg FGA', 'Avg FG%', 'Avg FT', 'Avg FTA',\n",
    "       'Avg 3PAr', 'Avg FTr', 'Avg PF', 'Avg PTS', 'Avg STL', 'Avg TOV',\n",
    "       'Avg TS%', 'Avg eFG%', 'Winning Percentage', 'oppAvg Win/Loss','oppAvg 3P',\n",
    "        'oppAvg 3PA', 'oppAvg 3P%', 'oppAvg Starter MP',\n",
    "        'oppAvg AST', 'oppAvg BLK', 'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG',\n",
    "        'oppAvg FGA', 'oppAvg FG%', 'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr',\n",
    "        'oppAvg FTr', 'oppAvg PF', 'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV',\n",
    "        'oppAvg TS%', 'oppAvg eFG%', 'oppWinning Percentage']]\n",
    "total_matrix = total_matrix.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = total_matrix[['Avg 3P%', 'Avg Starter MP', 'Avg AST', 'Avg ORB', 'Avg FG%', 'Avg FT', 'Avg FTA',\n",
    "                               'Avg 3PAr', 'Avg FTr', 'Avg PF', 'Avg TOV',\n",
    "                               'Avg TS%', 'Avg eFG%', 'Winning Percentage', 'oppAvg Win/Loss','oppAvg 3P',\n",
    "                               'oppAvg 3PA', 'oppAvg 3P%', 'oppAvg Starter MP',\n",
    "                               'oppAvg AST', 'oppAvg BLK', 'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG',\n",
    "                               'oppAvg FGA', 'oppAvg FG%', 'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr',\n",
    "                               'oppAvg FTr', 'oppAvg PF', 'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV',\n",
    "                               'oppAvg TS%', 'oppAvg eFG%', 'oppWinning Percentage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "results = total_matrix['+/-']\n",
    "#create target matrix \n",
    "# -1 = loss, 0 = tie, 1 = win\n",
    "temp_target = results.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(temp_target):\n",
    "    target = []\n",
    "    for result in temp_target:\n",
    "        if result < 0:\n",
    "            target.append(-1)\n",
    "        elif result == 0:\n",
    "            target.append(0)\n",
    "        else:\n",
    "            target.append(1)\n",
    "    return target\n",
    "target = create_target(temp_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing data\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(feature_matrix, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a decision tree model\n",
    "DTmodel = tree.DecisionTreeClassifier()\n",
    "DTmodel = DTmodel.fit(feature_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = DTmodel.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6050420168067226"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(target_test, predict)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter tuning for Decision Tree Classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.linspace(1, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = np.linspace(0.1, 1.0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = np.linspace(0.1, .5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 0\n",
    "s = 0\n",
    "l = 0 \n",
    "max_acc = 0\n",
    "best_tree = tree.DecisionTreeClassifier()\n",
    "for x in max_depths:\n",
    "    for y in min_samples_split:\n",
    "        for z in min_samples_leaf:\n",
    "                model = tree.DecisionTreeClassifier(max_depth=x, min_samples_split = y, min_samples_leaf=z)\n",
    "                model = model.fit(feature_train, target_train)\n",
    "                predict = model.predict(feature_test)\n",
    "                acc = accuracy_score(target_test, predict)\n",
    "                if acc > max_acc:\n",
    "                    max_acc = acc\n",
    "                    best_tree = model\n",
    "                    d, s, l = x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932773109243697"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 0.2, 0.1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, s, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below from: \n",
    "#https://stackoverflow.com/questions/25274673/is-it-possible-to-print-the-decision-tree-in-scikit-learn\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot \n",
    "\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(best_tree, out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "\n",
    "graph[0].write_pdf(\"tree.pdf\")\n",
    "\n",
    "#This creates a pdf showing the decision tree created by the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg 3P%\n",
      "Avg Starter MP\n",
      "Avg AST\n",
      "Avg ORB\n",
      "Avg FG%\n",
      "Avg FT\n",
      "Avg FTA\n",
      "Avg 3PAr\n",
      "Avg FTr\n",
      "Avg PF\n",
      "Avg TOV\n",
      "Avg TS%\n",
      "Avg eFG%\n",
      "Winning Percentage\n",
      "oppAvg Win/Loss\n",
      "oppAvg 3P\n",
      "oppAvg 3PA\n",
      "oppAvg Starter MP\n",
      "oppAvg AST\n",
      "oppAvg BLK\n",
      "oppAvg DRB\n",
      "oppAvg ORB\n",
      "oppAvg FG\n",
      "oppAvg FGA\n",
      "oppAvg FTA\n",
      "oppAvg 3PAr\n",
      "oppAvg FTr\n",
      "oppAvg PF\n",
      "oppAvg PTS\n",
      "oppAvg STL\n",
      "oppAvg TOV\n",
      "oppAvg TS%\n",
      "oppAvg eFG%\n",
      "oppWinning Percentage\n"
     ]
    }
   ],
   "source": [
    "important = DTmodel.feature_importances_\n",
    "len(important)\n",
    "cols = np.array(feature_matrix.columns)\n",
    "for c in cols:\n",
    "    if important[np.where(cols == c)] > 0:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Starter MP\n",
      "Avg PF\n",
      "Avg TOV\n",
      "Avg TS%\n",
      "Winning Percentage\n",
      "oppAvg Win/Loss\n",
      "oppWinning Percentage\n"
     ]
    }
   ],
   "source": [
    "#get important features from tree\n",
    "important = best_tree.feature_importances_\n",
    "len(important)\n",
    "cols = np.array(feature_matrix.columns)\n",
    "\n",
    "for c in cols:\n",
    "    if important[np.where(cols == c)] > 0:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avg 3P%', 'Avg Starter MP', 'Avg AST', 'Avg ORB', 'Avg FG%', 'Avg FT',\n",
       "       'Avg FTA', 'Avg 3PAr', 'Avg FTr', 'Avg PF', 'Avg TOV', 'Avg TS%',\n",
       "       'Avg eFG%', 'Winning Percentage', 'oppAvg Win/Loss', 'oppAvg 3P',\n",
       "       'oppAvg 3PA', 'oppAvg 3P%', 'oppAvg Starter MP', 'oppAvg AST',\n",
       "       'oppAvg BLK', 'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG', 'oppAvg FGA',\n",
       "       'oppAvg FG%', 'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr', 'oppAvg FTr',\n",
       "       'oppAvg PF', 'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV', 'oppAvg TS%',\n",
       "       'oppAvg eFG%', 'oppWinning Percentage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forest classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel = RandomForestClassifier(n_estimators = 100)\n",
    "RFmodel = RFmodel.fit(feature_train, target_train)\n",
    "RFpredict = RFmodel.predict(feature_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6890756302521008"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(target_test, RFpredict)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0\n",
    "est = 0\n",
    "for i in range(1,100):\n",
    "    RFmodel = RandomForestClassifier(n_estimators=i)\n",
    "    RFmodel = RFmodel.fit(feature_train, target_train)\n",
    "    RFpredict = RFmodel.predict(feature_test)\n",
    "    acc = accuracy_score(target_test, RFpredict)\n",
    "    if acc > maxAcc:\n",
    "        maxAcc = acc\n",
    "        est = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932773109243697\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "print(maxAcc)\n",
    "print(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.680672268907563"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFmodel = RandomForestClassifier(n_estimators=75)\n",
    "RFmodel = RFmodel.fit(feature_train, target_train)\n",
    "RFpredict = RFmodel.predict(feature_test)\n",
    "acc = accuracy_score(target_test, RFpredict)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_data(team):\n",
    "    stats = all_team_data[team]\n",
    "    stats = stats.dropna()\n",
    "    feature_matrix = stats[['Avg 3P%', 'Avg Starter MP', 'Avg AST', 'Avg ORB', 'Avg FG%', 'Avg FT', 'Avg FTA',\n",
    "                               'Avg 3PAr', 'Avg FTr', 'Avg PF', 'Avg TOV',\n",
    "                               'Avg TS%', 'Avg eFG%', 'Winning Percentage', 'oppAvg Win/Loss','oppAvg 3P',\n",
    "                               'oppAvg 3PA', 'oppAvg 3P%', 'oppAvg Starter MP',\n",
    "                               'oppAvg AST', 'oppAvg BLK', 'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG',\n",
    "                               'oppAvg FGA', 'oppAvg FG%', 'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr',\n",
    "                               'oppAvg FTr', 'oppAvg PF', 'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV',\n",
    "                               'oppAvg TS%', 'oppAvg eFG%', 'oppWinning Percentage']]\n",
    "    results = stats[['+/-']]\n",
    "    results = results.values\n",
    "    target = create_target(results)\n",
    "    return feature_matrix, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_season_data(team, year):\n",
    "    stats = all_team_data[team]\n",
    "    season_stats = stats.loc[stats['Season'] == year]\n",
    "    season_stats = season_stats.dropna()\n",
    "    feature_matrix = season_stats[['Avg 3P%', 'Avg Starter MP', 'Avg AST', 'Avg ORB', 'Avg FG%', 'Avg FT', 'Avg FTA',\n",
    "                               'Avg 3PAr', 'Avg FTr', 'Avg PF', 'Avg TOV',\n",
    "                               'Avg TS%', 'Avg eFG%', 'Winning Percentage', 'oppAvg Win/Loss','oppAvg 3P',\n",
    "                               'oppAvg 3PA', 'oppAvg 3P%', 'oppAvg Starter MP',\n",
    "                               'oppAvg AST', 'oppAvg BLK', 'oppAvg DRB', 'oppAvg ORB', 'oppAvg FG',\n",
    "                               'oppAvg FGA', 'oppAvg FG%', 'oppAvg FT', 'oppAvg FTA', 'oppAvg 3PAr',\n",
    "                               'oppAvg FTr', 'oppAvg PF', 'oppAvg PTS', 'oppAvg STL', 'oppAvg TOV',\n",
    "                               'oppAvg TS%', 'oppAvg eFG%', 'oppWinning Percentage']]\n",
    "    results = season_stats[['+/-']]\n",
    "    results = results.values\n",
    "    target = create_target(results)\n",
    "    return feature_matrix, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_model(team, year):\n",
    "    feature_matrix, target = get_team_season_data(team, year)\n",
    "    f_train, f_test, t_train, t_test = train_test_split(feature_matrix, target, test_size=0.5, random_state=42)\n",
    "    model = tree.DecisionTreeClassifier(max_depth=d, min_samples_split =s, min_samples_leaf=l)\n",
    "    model = model.fit(f_train, t_train)\n",
    "    predict = model.predict(f_test)\n",
    "    acc = accuracy_score(t_test, predict)\n",
    "    \n",
    "    #get important features for the season\n",
    "    important = model.feature_importances_\n",
    "    cols = np.array(feature_matrix.columns)\n",
    "    print(team + \" \" + year + \":\")\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    print(\"Important Features:\")\n",
    "    for c in cols:\n",
    "        if important[np.where(cols == c)] > 0:\n",
    "            print('\\t' + c)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philadelphia 2007:\n",
      "Accuracy: 0.6666666666666666\n",
      "Important Features:\n",
      "\toppAvg Starter MP\n",
      "\toppAvg PF\n",
      "\toppAvg PTS\n",
      "\toppAvg eFG%\n",
      "\n",
      "\n",
      "Philadelphia 2008:\n",
      "Accuracy: 0.4878048780487805\n",
      "Important Features:\n",
      "\tAvg ORB\n",
      "\tAvg TOV\n",
      "\tAvg TS%\n",
      "\toppAvg Win/Loss\n",
      "\toppAvg PF\n",
      "\n",
      "\n",
      "Philadelphia 2009:\n",
      "Accuracy: 0.6216216216216216\n",
      "Important Features:\n",
      "\tAvg PF\n",
      "\toppAvg AST\n",
      "\toppAvg ORB\n",
      "\toppAvg FG\n",
      "\n",
      "\n",
      "Philadelphia 2010:\n",
      "Accuracy: 0.5897435897435898\n",
      "Important Features:\n",
      "\tWinning Percentage\n",
      "\toppAvg FG%\n",
      "\n",
      "\n",
      "Philadelphia 2011:\n",
      "Accuracy: 0.5945945945945946\n",
      "Important Features:\n",
      "\tAvg TOV\n",
      "\toppAvg eFG%\n",
      "\toppWinning Percentage\n",
      "\n",
      "\n",
      "Philadelphia 2012:\n",
      "Accuracy: 0.358974358974359\n",
      "Important Features:\n",
      "\tAvg eFG%\n",
      "\toppAvg BLK\n",
      "\toppAvg FG\n",
      "\n",
      "\n",
      "Philadelphia 2013:\n",
      "Accuracy: 0.675\n",
      "Important Features:\n",
      "\tAvg FTr\n",
      "\toppAvg Win/Loss\n",
      "\toppAvg FTA\n",
      "\n",
      "\n",
      "Philadelphia 2014:\n",
      "Accuracy: 0.5853658536585366\n",
      "Important Features:\n",
      "\toppAvg Win/Loss\n",
      "\toppAvg 3P%\n",
      "\toppAvg BLK\n",
      "\n",
      "\n",
      "Philadelphia 2015:\n",
      "Accuracy: 0.8048780487804879\n",
      "Important Features:\n",
      "\tAvg AST\n",
      "\toppWinning Percentage\n",
      "\n",
      "\n",
      "Philadelphia 2016:\n",
      "Accuracy: 0.5365853658536586\n",
      "Important Features:\n",
      "\tAvg FG%\n",
      "\toppAvg Starter MP\n",
      "\toppAvg BLK\n",
      "\n",
      "\n",
      "Philadelphia 2017:\n",
      "Accuracy: 0.6739130434782609\n",
      "Important Features:\n",
      "\tAvg ORB\n",
      "\tAvg FTA\n",
      "\tAvg 3PAr\n",
      "\tAvg TOV\n",
      "\n",
      "\n",
      "Philadelphia 2018:\n",
      "Accuracy: 0.5526315789473685\n",
      "Important Features:\n",
      "\tAvg 3PAr\n",
      "\toppAvg 3P\n",
      "\toppAvg 3PA\n",
      "\toppAvg PF\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    season_model('Philadelphia', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try model on other teams\n",
    "def predict_team(model, team):\n",
    "    feature, target = get_team_data(team)\n",
    "    predict = model.predict(feature)\n",
    "    acc = accuracy_score(target, predict)\n",
    "    print(team + \": \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York: 0.5692963752665245\n",
      "Milwaukee: 0.5264270613107822\n",
      "L.A. Clippers: 0.601842374616172\n",
      "San Antonio: 0.5771428571428572\n",
      "Indiana: 0.5491388044579534\n",
      "Minnesota: 0.6409978308026031\n",
      "Chicago: 0.574468085106383\n",
      "Charlotte: 0.588855421686747\n",
      "Philadelphia: 0.6789473684210526\n",
      "Brooklyn: 0.5723684210526315\n",
      "Houston: 0.5869346733668341\n",
      "New Orleans: 0.49375866851595007\n",
      "Memphis: 0.5667686034658511\n",
      "Oklahoma City: 0.6247357293868921\n",
      "Utah: 0.5799793601651186\n",
      "Toronto: 0.6086508753861998\n",
      "Orlando: 0.5915637860082305\n",
      "Detroit: 0.5615711252653928\n",
      "Golden State: 0.5616438356164384\n",
      "Phoenix: 0.6240681576144835\n",
      "Denver: 0.570385818561001\n",
      "Atlanta: 0.6067527308838133\n",
      "Sacramento: 0.5610021786492375\n",
      "Cleveland: 0.6085271317829457\n",
      "L.A. Lakers: 0.5816733067729084\n",
      "Boston: 0.586565752128666\n",
      "Washington: 0.6058394160583942\n",
      "Portland: 0.572463768115942\n",
      "Dallas: 0.6069600818833163\n",
      "Miami: 0.5672514619883041\n"
     ]
    }
   ],
   "source": [
    "for team in teams:\n",
    "    predict_team(tree_model, team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_plot_season(feature, target):\n",
    "    predict = RFmodel.predict(feature)\n",
    "    correctness = []\n",
    "    for i in predict:\n",
    "        if i == target[i]:\n",
    "            correctness.append(1)\n",
    "        else:\n",
    "            correctness.append(0)\n",
    "    game_count = np.linspace(1, len(predict), len(predict))\n",
    "    plt.scatter(game_count, correctness)\n",
    "    plt.show()\n",
    "    print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
