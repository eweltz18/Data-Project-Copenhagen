{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: UserWarning: You do not have a working installation of the service_identity module: 'No module named 'service_identity''.  Please install it from <https://pypi.python.org/pypi/service_identity> and make sure all of its dependencies are satisfied.  Without the service_identity module, Twisted can perform only rudimentary TLS client hostname verification.  Many valid certificate/hostname mappings may be rejected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re\n",
    "import time\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make month numbers \n",
    "def two_digits(x):\n",
    "    if int(x)<10:\n",
    "        return str(0) + str(x)\n",
    "    else:\n",
    "        return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to convert time to numeric value\n",
    "def get_min(time_str):\n",
    "    m, s = time_str.split(':')\n",
    "    return int(m) + int(s)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting 5 get a 1, reserves a 0\n",
    "def starter(x):\n",
    "    if (x<5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "starter_v = np.vectorize(starter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to determine the winner\n",
    "def winner(x, y):\n",
    "    home = False\n",
    "    x['PTS'] = x['PTS'].fillna(0)\n",
    "    y['PTS'] = y['PTS'].fillna(0)\n",
    "    home_score = x['PTS'].astype(int).sum()\n",
    "    away_score = y['PTS'].astype(int).sum()\n",
    "    dif = home_score-away_score\n",
    "    if (dif>=0):\n",
    "        win = x['Team'][0]\n",
    "        los = y['Team'][0]\n",
    "        home = True\n",
    "    else:\n",
    "        los = x['Team'][0]\n",
    "        win = y['Team'][0]\n",
    "    return(win, los, home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to turn scraped data into a dataframe\n",
    "def create_df(basic_headers, advanced_headers, basic_values, advanced_values):\n",
    "    df1 = pd.DataFrame(basic_values, columns = basic_headers)\n",
    "    df1 = df1.replace(to_replace = '[^>]+>', value='', regex=True) # clean data\n",
    "    df1['MP'] = df1['MP'].replace('', '0:0')\n",
    "    df1 = df1.replace('', 0) #clean data\n",
    "    df1['MP'] = df1['MP'].apply(get_min)\n",
    "    df1['Starter'] = starter_v(df1.index.values) # reports if the player is a starter \n",
    "    df2 = pd.DataFrame(advanced_values, columns = advanced_headers)\n",
    "    df2 = df2.replace(to_replace = '[^>]+>', value='', regex=True)\n",
    "    df2['MP'] = df2['MP'].replace('', '0:0')\n",
    "    df2 = df2.replace('', 0) #clean data\n",
    "    df2['MP'] = df2['MP'].apply(get_min)\n",
    "    df2['Starter'] = starter_v(df2.index.values) # reports if the player is a starter \n",
    "    return pd.merge(df1, df2) #returns one merged df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fucntion to get the wins and losses of each team\n",
    "def add_teams_and_records(home, away, home_df, away_df):\n",
    "    reg = r\"^([\\w]+ [\\w]+ [a-zA-Z]*)\"\n",
    "    home_team = re.findall(reg, home)[0]\n",
    "    record1 = home[home.find(\"(\")+1:home.find(\")\")]\n",
    "    home_win = int(record1[0:record1.find(\"-\")])\n",
    "    home_loss = int(record1[record1.find(\"-\")+1:])\n",
    "    away_team = re.findall(reg, away)[0]\n",
    "    record2 = away[away.find(\"(\")+1:away.find(\")\")]\n",
    "    away_win = int(record2[0:record2.find(\"-\")])\n",
    "    away_loss = int(record2[record2.find(\"-\")+1:])\n",
    "    home_df['Team'] = home_team\n",
    "    away_df['Team'] = away_team\n",
    "    win, los, home = winner(home_df, away_df)\n",
    "    if(home):\n",
    "        home_win -= 1\n",
    "        away_loss -= 1\n",
    "    else:\n",
    "        home_loss -= 1\n",
    "        away_win -= 1\n",
    "    home_df['Wins'] = home_win\n",
    "    home_df['Losses'] = home_loss \n",
    "    away_df['Wins'] = away_win\n",
    "    away_df['Losses'] = away_loss \n",
    "    return home_df, away_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine individual player stats into team stats\n",
    "def compress(home):\n",
    "    home = home.fillna(0)\n",
    "    home_compressed = pd.Series()\n",
    "    home_compressed['Team'] = home['Team'][0]\n",
    "    home_compressed['Wins'] = home['Wins'][0]\n",
    "    home_compressed['Losses'] = home['Losses'][0]\n",
    "    home_compressed['Year'] = home['Year'][0]\n",
    "    home_compressed['Month'] = home['Month'][0]\n",
    "    home_compressed['Day'] = home['Day'][0]\n",
    "    home_compressed['Starters MP'] = home['MP'][:5].mean()\n",
    "    home_compressed['FG'] = home['FG'].astype(int).sum()\n",
    "    home_compressed['FGA'] = home['FGA'].astype(int).sum()\n",
    "    home_compressed['FG%'] = home_compressed['FG']/home_compressed['FGA']\n",
    "    home['3P'] = home['3P'].replace('',0)\n",
    "    home['3PA'] = home['3PA'].replace('',0)\n",
    "    home_compressed['3P'] = home['3P'].astype(int).sum()\n",
    "    home_compressed['3PA'] = home['3PA'].astype(int).sum()\n",
    "    home_compressed['3P%'] = home_compressed['3P']/home_compressed['3PA']\n",
    "    home_compressed['FT'] = home['FT'].astype(int).sum()\n",
    "    home_compressed['FTA'] = home['FTA'].astype(int).sum()\n",
    "    home_compressed['FT%'] = home_compressed['FT']/home_compressed['FTA']\n",
    "    home_compressed['ORB'] = home['ORB'].astype(int).sum()\n",
    "    home_compressed['DRB'] = home['DRB'].astype(int).sum()\n",
    "    home_compressed['AST'] = home['AST'].astype(int).sum()\n",
    "    home_compressed['STL'] = home['STL'].astype(int).sum()\n",
    "    home_compressed['BLK'] = home['BLK'].astype(int).sum()\n",
    "    home_compressed['TOV'] = home['TOV'].astype(int).sum()\n",
    "    home_compressed['PF'] = home['PF'].astype(int).sum()\n",
    "    home_compressed['PTS'] = home['PTS'].replace('', 0, regex=True).astype(int).sum()\n",
    "    home['+/-'] = home['+/-'].replace('', 0, regex=True)\n",
    "    home_compressed['+/-'] = home['+/-'].astype(int).sum()\n",
    "    home_compressed['TS%'] = home['PTS'].astype(int).sum()/(2*(home['FGA'].astype(int).sum()+.44*home['FTA'].astype(int).sum()))\n",
    "    home_compressed['eFG%'] = (home['FG'].astype(int).sum()+ .5 * home['3P'].astype(int).sum())/home['FGA'].astype(int).sum()\n",
    "    home_compressed['3PAr'] = home_compressed['3P']/home_compressed['FGA']\n",
    "    home_compressed['FTr'] = home_compressed['FT']/home_compressed['FGA']\n",
    "    return home_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#account for team name changes\n",
    "def update(date, team):\n",
    "    if date < '200809':\n",
    "        team = team.replace('cho', 'cha')\n",
    "        team = team.replace('okc', 'sea')\n",
    "        team = team.replace('brk', 'njn')\n",
    "        team = team.replace('nop', 'noh')\n",
    "    elif date < '201209':\n",
    "        team = team.replace('cho', 'cha')\n",
    "        team = team.replace('brk', 'njn')\n",
    "        team = team.replace('nop', 'noh')\n",
    "    elif date < '201309':\n",
    "        team = team.replace('cho', 'cha')\n",
    "        team = team.replace('nop', 'noh')\n",
    "    elif date < '201409':\n",
    "        team = team.replace('cho', 'cha')\n",
    "    return team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of teams abbreviations\n",
    "teams = ['ATL', 'BKN', 'BOS', 'CHO', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU',\n",
    "         'IND','LAC','LAL','MEM','MIA','MIL','MIN','NOP','NYK','OKC','ORL','PHI','PHO',\n",
    "         'POR','SAC','SAS','TOR','UTA','WAS', 'SEA', 'CHA', 'NJN', 'NOH', 'BRK']\n",
    "#list of teams\n",
    "long_teams = ['Atlanta Hawks', 'Brooklyn Nets', 'Boston Celtics', 'Charlotte Hornets', 'Chicago Bulls',\n",
    "              'Cleveland Cavaliers', 'Dallas Mavericks', 'Denver Nuggets', 'Detroit Pistons', 'Golden State Warriors',\n",
    "              'Houston Rockets', 'Indiana Pacers', 'Los Angeles Clippers', 'Los Angeles Lakers',\n",
    "              'Memphis Grizzlies', 'Miami Heat', 'Milwaukee Bucks', 'Minnesota Timberwolves',\n",
    "              'New Orleans Pelicans', 'New York Knicks', 'Oklahoma City Thunder', 'Orlando Magic',\n",
    "              'Philadelphia 76ers', 'Phoenix Suns', 'Portland Trail Blazers', 'Sacramento Kings',\n",
    "              'San Antonio Spurs', 'Toronto Raptors', 'Utah Jazz', 'Washington Wizards', 'Seattle SuperSonics', \n",
    "              'Charlotte Bobcats', 'New Jersey Nets', 'New Orleans Hornets', 'Brooklyn Nets']\n",
    "abr = dict(zip(long_teams,teams))\n",
    "odds_df = pd.read_csv('oddsdf.csv', encoding = \"ISO-8859-1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split by home or away\n",
    "away_teams = odds_df[odds_df.index%2==0]['Team']\n",
    "home_teams = odds_df[odds_df.index%2==1]['Team']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Atlanta', 'Boston', 'Brooklyn', 'Charlotte', 'Chicago', 'Cleveland', 'Dallas', 'Denver', 'Detroit', 'Golden State', 'Houston', 'Indiana', 'L.A. Clippers', 'L.A. Lakers', 'Memphis', 'Miami', 'Milwaukee', 'Minnesota', 'New Orleans', 'New York', 'Oklahoma City', 'Orlando', 'Philadelphia', 'Phoenix', 'Portland', 'Sacramento', 'San Antonio', 'Toronto', 'Utah', 'Washington']\n"
     ]
    }
   ],
   "source": [
    "x = list(set(list(home_teams)+ list(away_teams)))\n",
    "x.sort()\n",
    "for y in ['Baltimore', 'East', 'West', 'Pittsburgh', 'Maccabi Haifa Bc','Louisville', 'Miami (FL)', 'Team Giannis', 'USA All Stars',\n",
    "          'Team LeBron', 'Team Stephen', 'Team USA', 'Team World', 'World All Stars', 'Team LeBron ']:\n",
    "    x.remove(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = ['ATL', 'BOS', 'BRK', 'CHO', 'CHI', 'CLE', 'DAL', 'DEN', 'DET', 'GSW', 'HOU',\n",
    "         'IND','LAC','LAL','MEM','MIA','MIL','MIN','NOP','NYK','OKC','ORL','PHI','PHO',\n",
    "         'POR','SAC','SAS','TOR','UTA','WAS']\n",
    "abr2 = dict(zip(x, teams)) # zip the team names from the odds df and the abreviations for the website url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urltemplate = \"https://www.basketball-reference.com/boxscores/{year}{month}{day}0{team}.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only include valid home teams and clean data\n",
    "home_teams_df = odds_df[odds_df.index%2==1]\n",
    "home_teams_df = home_teams_df[home_teams_df['Team'].isin(x)]\n",
    "teams = home_teams_df['Team']\n",
    "home_teams_df['yearmonth'] = home_teams_df['Year'].astype(str) + home_teams_df['Month'].apply(two_digits)\n",
    "abreviations = [abr2[x] for x in teams]\n",
    "abreviations = [update(x[0],x[1]) for x in list(zip(home_teams_df['yearmonth'].tolist(), abreviations))]\n",
    "home_teams_df['Abr'] = abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only include valid away teams and clean data\n",
    "away_teams_df = odds_df[odds_df.index%2==0]\n",
    "away_teams_df = away_teams_df[away_teams_df['Team'].isin(x)]\n",
    "teams = away_teams_df['Team']\n",
    "away_teams_df['yearmonth'] = away_teams_df['Year'].astype(str) + away_teams_df['Month'].apply(two_digits)\n",
    "abreviations = [abr2[x].lower() for x in teams]\n",
    "abreviations = [update(x[0], x[1]) for x in list(zip(away_teams_df['yearmonth'].tolist(), abreviations))]\n",
    "away_teams_df['Abr'] = abreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_teams_df = home_teams_df[['Team', 'Year', 'Month', 'Day', 'Abr']]\n",
    "home_teams_df['URL'] = 'https://www.basketball-reference.com/boxscores/' + home_teams_df['Year'].astype(str) + home_teams_df['Month'].apply(two_digits) + home_teams_df['Day'].apply(two_digits) + '0' + home_teams_df['Abr'] + '.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = home_teams_df['URL']\n",
    "away_team_dict = dict(zip(urls, away_teams_df['Abr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 19:14:39 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-04-09 19:14:39 [scrapy.utils.log] INFO: Versions: lxml 3.8.0.0, libxml2 2.9.9, cssselect 1.0.1, parsel 1.5.1, w3lib 1.17.0, Twisted 17.5.0, Python 3.6.8 |Anaconda custom (x86_64)| (default, Dec 29 2018, 19:04:46) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-17.6.0-x86_64-i386-64bit\n",
      "2019-04-09 19:14:39 [scrapy.crawler] INFO: Overridden settings: {'FEED_FORMAT': 'csv', 'FEED_URI': 'games_data.csv', 'LOG_LEVEL': 30}\n",
      "2019-04-09 20:04:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.basketball-reference.com/boxscores/201102130MEM.html> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4294, in create_block_manager_from_blocks\n",
      "    placement=slice(0, len(axes[0])))]\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 2719, in make_block\n",
      "    return klass(values, ndim=ndim, fastpath=fastpath, placement=placement)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 115, in __init__\n",
      "    len(self.mgr_locs)))\n",
      "ValueError: Wrong number of items passed 20, placement implies 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-19-6073abc4ec07>\", line 41, in parse\n",
      "    away_df = create_df(away_column_headers, away_column_headers_adv, away_column_data, away_column_data_adv)\n",
      "  File \"<ipython-input-6-313202c7cda4>\", line 3, in create_df\n",
      "    df1 = pd.DataFrame(basic_values, columns = basic_headers)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\", line 306, in __init__\n",
      "    copy=copy)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\", line 483, in _init_ndarray\n",
      "    return create_block_manager_from_blocks([values], [columns, index])\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4303, in create_block_manager_from_blocks\n",
      "    construction_error(tot_items, blocks[0].shape[1:], axes, e)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4278, in construction_error\n",
      "    raise ValueError(\"Empty data passed with indices specified.\")\n",
      "ValueError: Empty data passed with indices specified.\n",
      "2019-04-09 20:04:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.basketball-reference.com/boxscores/201102130DET.html> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4294, in create_block_manager_from_blocks\n",
      "    placement=slice(0, len(axes[0])))]\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 2719, in make_block\n",
      "    return klass(values, ndim=ndim, fastpath=fastpath, placement=placement)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 115, in __init__\n",
      "    len(self.mgr_locs)))\n",
      "ValueError: Wrong number of items passed 20, placement implies 0\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/utils/defer.py\", line 102, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/referer.py\", line 339, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"<ipython-input-19-6073abc4ec07>\", line 41, in parse\n",
      "    away_df = create_df(away_column_headers, away_column_headers_adv, away_column_data, away_column_data_adv)\n",
      "  File \"<ipython-input-6-313202c7cda4>\", line 3, in create_df\n",
      "    df1 = pd.DataFrame(basic_values, columns = basic_headers)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\", line 306, in __init__\n",
      "    copy=copy)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\", line 483, in _init_ndarray\n",
      "    return create_block_manager_from_blocks([values], [columns, index])\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4303, in create_block_manager_from_blocks\n",
      "    construction_error(tot_items, blocks[0].shape[1:], axes, e)\n",
      "  File \"/Users/ericweltz/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\", line 4278, in construction_error\n",
      "    raise ValueError(\"Empty data passed with indices specified.\")\n",
      "ValueError: Empty data passed with indices specified.\n"
     ]
    }
   ],
   "source": [
    "#scrapy class to find all game data\n",
    "#use time to determine function length\n",
    "t = time.time()\n",
    "class GameScraper(scrapy.Spider):\n",
    "    name = \"GameData\"\n",
    "    start_urls = urls\n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.WARNING,\n",
    "        'FEED_FORMAT':'csv',\n",
    "        'FEED_URI': 'games_data.csv',\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        day = response.url[-11:-9]\n",
    "        month = response.url[-13:-11]\n",
    "        year = response.url[-17:-13]\n",
    "        away = away_team_dict[response.url]\n",
    "        home = response.url[-8:-5].lower()\n",
    "\n",
    "\n",
    "        #gets the basic column headers and advanced headers for each team\n",
    "        away_column_headers = [item[1:-1] for item in response.xpath(\"//table[contains (@id, 'box_{team}_basic')]/thead\".format(team=away)).re('>.+<')][3:]\n",
    "        away_column_headers_adv = [item[1:-1] for item in response.xpath(\"//table[contains (@id, 'box_{team}_advanced')]/thead\".format(team=away)).re('>.+<')][3:]\n",
    "        home_column_headers = [item[1:-1] for item in response.xpath(\"//table[contains (@id, 'box_{team}_basic')]/thead\".format(team=home)).re('>.+<')][3:]\n",
    "        home_column_headers_adv = [item[1:-1] for item in response.xpath(\"//table[contains (@id, 'box_{team}_advanced')]/thead\".format(team=home)).re('>.+<')][3:]\n",
    "        #get player data\n",
    "        away_column_data = [item for item in response.xpath(\"//table[contains (@id, 'box_{team}_basic')]/tbody/tr/td\".format(team=away)).re('data-stat[^<]+')]\n",
    "        away_column_data_adv = [item for item in response.xpath(\"//table[contains (@id, 'box_{team}_advanced')]/tbody/tr/td\".format(team=away)).re('data-stat[^<]+')]\n",
    "        home_column_data = [item for item in response.xpath(\"//table[contains (@id, 'box_{team}_basic')]/tbody/tr/td\".format(team=home)).re('data-stat[^<]+')]\n",
    "        home_column_data_adv = [item for item in response.xpath(\"//table[contains (@id, 'box_{team}_advanced')]/tbody/tr/td\".format(team=home)).re('data-stat[^<]+')]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #reshape data from list into 2d matrix\n",
    "        away_column_data = np.array([y for y in away_column_data if 'data-stat=\"reason\"' not in y]).reshape((int(len(away_column_data)/20),20))\n",
    "        away_column_data_adv = np.array([y for y in away_column_data_adv if 'data-stat=\"reason\"' not in y]).reshape((int(len(away_column_data_adv)/15),15))\n",
    "        \n",
    "        home_column_data = np.array([y for y in home_column_data if 'data-stat=\"reason\"' not in y]).reshape((int(len(home_column_data)/20),20))\n",
    "        home_column_data_adv = np.array([y for y in home_column_data_adv if 'data-stat=\"reason\"' not in y]).reshape((int(len(home_column_data_adv)/15),15))\n",
    "        \n",
    "        \n",
    "        away_df = create_df(away_column_headers, away_column_headers_adv, away_column_data, away_column_data_adv)\n",
    "        home_df = create_df(home_column_headers, home_column_headers_adv, home_column_data, home_column_data_adv)\n",
    "        \n",
    "        away_df['Year'] = year\n",
    "        away_df['Month'] = month\n",
    "        away_df['Day'] = day\n",
    "        home_df['Year'] = year\n",
    "        home_df['Month'] = month\n",
    "        home_df['Day'] = day\n",
    "        \n",
    "        \n",
    "        home_df, away_df = add_teams_and_records(response.xpath(\"//div[contains(@id, 'all_box_{team}_basic')]//h2/text()\".format(team=home)).get(),\n",
    "                                                                    response.xpath(\"//div[contains(@id, 'all_box_{team}_basic')]//h2/text()\".format(team=away)).get(), home_df, away_df)\n",
    "        \n",
    "        home_df = compress(home_df)\n",
    "        away_df = compress(away_df)\n",
    "        \n",
    "        home_df['Code'] = home + away\n",
    "        away_df['Code'] = home + away\n",
    "        home_df['Home'] = 1\n",
    "        away_df['Home'] = 0\n",
    "        \n",
    "    \n",
    "        \n",
    "        yield home_df.to_csv('Games/{team}{year}{month}{day}.csv'.format(team = home, day = day, month = month, year = year))\n",
    "        yield away_df.to_csv('Games/{team}{year}{month}{day}.csv'.format(team = away, day = day, month = month, year = year))\n",
    "        \n",
    "process = CrawlerProcess()\n",
    "process.crawl(GameScraper)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8363.233618736267\n"
     ]
    }
   ],
   "source": [
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
